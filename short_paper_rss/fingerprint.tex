\documentclass[conference]{IEEEtran}
\usepackage{times}

% numbers option provides compact numerical references in the text. 
\usepackage[numbers]{natbib}
\usepackage{multicol}
\usepackage[bookmarks=true]{hyperref}

\title{Identification Through Interaction}
\author{AUTHORS \meta{TODO: LIST AUTHORS}} %(TODO: Elaine or Matthias last author? Up to Matthias, send over before submission. )
\date{\vspace{-1em}}


\usepackage{natbib}
\usepackage{graphicx}
\usepackage{color}
\usepackage[normalem]{ulem}
\newcommand{\elaine}[1]{{\textcolor[rgb]{0.1,0.4,0.6}{[ESS: {\it #1}]}}}
\newcommand{\delete}[1]{{\textcolor[rgb]{0.75,0.1,0}{\sout{#1}}}}
\newcommand{\meta}[1]{{\textcolor[rgb]{0.1,0.7,0.2}{[JSS: {\it #1}]}}}

\begin{document}

\maketitle

% for final report only
%\begin{abstract}
%A one paragraph high level description of the work. The abstract is typically the first thing someone would read from the paper before deciding whether to continue reading and hence, serves as an advertisement to the reader to read the whole paper. 
%\end{abstract}

\section{Introduction}
Why do we care? Why is it hard? Why can we do it?\\

Recognizing individuals is critical to forming strong human-agent relationships. A person benefits from being recognized in several ways. An agent can offer a recognized person customized behaviors based on previous encounters to facilitate the interaction. Recognition breeds familiarity with people. Most interestingly, if the agent can recognize the person through a modality that is not readily transferrable to another device, the person may experience the benefits of recognition without the privacy and security drawbacks inherent in a biometric fingerprint. 

Modern person identification is accomplished through face or voice recognition, but these modalities are sensitive to environmental conditions and may have stringent set-up requirements. Not every interactive device is able to meet these requirements -- for example a vacuum robot is too low to the ground to obtain a clear shot of a person's face, a tabletop robot needs to be responsive to vocal commands even when the person is turned away, and an exercise watch may be used on a loud, chaotic city street. 

Even if it was possible to use voice and face recognition in all contexts it would not necessarily be desirable. A facial or vocal fingerprint can be easily transferred between devices that have cameras and microphones. Privacy conscious individuals are aware that the security of their identifying data is entirely in the hands of the companies supplying their technology. 

Identification schemes based around behavior can offer a solution to this privacy concern by making the identification contingent on the interaction between the agent and the person. The fingerprint left by an interaction can not be recreated by a passive device as it requires action and reaction between two parties. It may be the case that the \textit{interaction fingerprint} generated by a specific instance of an agent is usable only by an agent of the same type, configured in the same way. This kind of identification would be harder to share and so would protect the privacy concerns of the user without sacrificing the benefits of human-agent identification. 

In addition, interaction fingerprinting generalizes across many types of interactive systems. It sits atop the existing infrastructure between the person and the agent, and could be incorporated directly into the user-interface. 

% Some applications, like Amazon's Smart Speaker, can get around this with hardware design and big-data enabled machine learning, but this is not possible for many organizations and may not be a desired feature from the user's perspective. Robust voice and face recognition allow a user to be tracked across multiple contexts, but identification through interaction isn't likely to be reusable in the same way, giving the user the benefits of personalized interaction without further compromising their right to privacy.

% \meta{Be very specific about constraints. "if you have a tabletop robot in a person's house, you might not be able to point it at your face.". Very specific examples/claims. There are state of the art systems that to face and voice detection, but you need specific views, specific hardware. You need some combination of environment and hardware that is conducive to this detection. Not every interactive device is able to meet this requirements -- for example (1) roomba blah (2) tabletop robot (3) third example -- make this one sentence. Going into detail on this increases your attack surface area. HARDWARE LIMITATIONS ARE NOT AN INTERESTING MOTIVATION FOR A PAPER}
% There are state-of-the-art systems that perform accurate voice and face recognition in the whil 
% It's unreasonable to expect every piece of interactive technology to implement robust person identification systems given their requirements. However, most embodied piece of interactive technology can benefit from remembering previous interactions with individual users, and so a more reliable, scalable \meta{setting yourself up to have to prove reliability and scalability. implicit claim we don't want to prove} person recognition scheme is needed. 

In this paper, we test whether modality-agnostic features such as average signal activity and oscillation can be used for person identification. This lays the groundwork for an alternate identification system that can be used with any socially interactive device by utilizing the interaction itself to recognize individuals. For example, we use the embedding of the behavior in time as a way to represent interactions without requiring prior knowledge about the meaning of each signal stream, a change from prior work in open-set recognition which has only used static features of the user. Prior work in open-set recognition has only used temporally static features of the user, but a key feature of an interaction is that it includes behaviors that are embedded in time. In this work we present a novel method for doing person recognition on time-series data with support vector machines, and discuss how this work can be extended to address the problem of recognition through interaction as well as open-set recognition. We evaluate our system on the AMI corpus \cite{kilgour_ami_nodate} and discuss how it could be extended in a case where the agent is able to take action and elicit behavioral responses from a user. 


% \elaine{note that this specifies what sub-part of the problem you're addressing; if you want to address a different sub-part, you'll need to phrase it differently}
% We build upon previous work in open-set recognition to accomplish this, and extend that work to use time-series interaction data. \elaine{Rephrase to be something more like "Prior work in open-set recognition has only used static features of the user, but a key feature of an interaction is that it includes behaviors that are embedded in time.  In this work we present a novel method for doing open-set recognition on time-series data, and discuss how this can be extended to address the problem of recognition from interaction. -- we evaluate it on a preexisting dataset and discuss how it could be extended in a case where the agent is able to take action. -- OR -- in this work we show that these techniques can be effectively applied to time-series data and discuss how the problem can be extended if the agent can take actions."}


% -- if you know how to do all the things. Put together the part that exist to accomplish the task, discuss what problems are unsolved w/r to system -- 

% Knowing that I have a bunch of events of different types in time, without knowing the types, can we use them to recognize the people. 

% 1) Cut things into snippets, can you identify people between snippets? Training set, Test set, Validation set. 

% 2) Between interactions, can you recognize that these are all new people? 

% Hand-waving timing of events. How do you turn an event that takes place over time into a datapoint that an SVM can handle. **Distance are further in higher dimensions***

% TODO: It works and then I write a paragraph about how well it works.

% 3) *IDEA* Turn temporal patterns into image patterns and then analyze with computer vision. Like looking at a fingerprint. 


% -- 1. Turn the time-series data into something an SVM can handle. 2. show if you can classify people. -- show preliminary results, talk about why its hard, talk about what hasn't been done yet, talk about how the robot can elicit interaction to identify, EXPLICITLY tie this into robots in the world. 

% FIRST GOAL: finger-print and a couple graphs. 

\section{Background Related Work}


All person recognition systems extract features from the target modality to run through a classifier. Face recognition systems extract computable features through classical computer vision \cite{turk1991face} or with modern neural networks \cite{parkhi2015deep} to learn classification models for known individuals. Similarly, voice recognition systems generate fingerprints for users by extracting features based on different auditory domains like time and frequency\cite{alonso2014speaker}. Please see Tolba et al. \cite{tolba2006face} and Beigi et al. \cite{beigi2011speaker} for literature reviews of face and voice recognition, respectively. 

Alternate identification schemes, like gait detection, require the user's full body to be recorded for multiple striding frames \cite{liang_wang_silhouette_2003}\cite{han_individual_2006}. Gait detection is an effective means of closed set person recognition \cite{tian2019free}, but infeasible for many pieces of interactive technology because of the perspective on the user required to observe their gait.

In addition to hard biometrics like face, voice, and gait features, soft biometrics can improve identification rates. Jain et al. \cite{jain2004soft} showed how a combination of soft biometrics like height, weight, and age could be used to augment a primary identification system for a closed set of individuals. Multi-modal systems using soft biometrics have been applied to open set identification problems as well with mixed results. \cite{irfan2018multi}\cite{martinson2013identifying}  

A multi-modal approach can be applied to behavior as well. Perez et al. showed that social media metadata, which can be considered to be a behavioral signature, was effective at accurately identifying individuals in closed groups of up to 10,000 people \cite{perez_you_2018}. Their accuracy was due in part to the large number of raw features available -- the meta data features could be used without processing -- but this work shows that high degrees of accuracy are possible with out of the box classifiers if an identifying set of behavioral features can be extracted. 

We looked to open-set classification theory for hints on identifying people through their behavioral patterns since we prefer to avoid a standard behavioral fingerprint learning routine of the kind used for face and voice identification. Scheirer et al. uses SVMs augmented with a measure of open-space risk to determine how a new sample should be classified\cite{scheirer_toward_2013}. Based on Scheier et al. and Bendale et al's promising results with SVMs on open-set problems \cite{bendale_towards_2015}\cite{scheirer_toward_2013}, we used a multi-class SVM trained on multiple labelled snippets of interaction. In this work, we focus on distinguishing between four known participants in an ongoing interaction, leaving the open-set extension of this to future work.
% TODO: Don't write a book report. Only bring things up as they relate to our formalization of the problem. (1) work on multi-class SVM and where we fit in. (2) How multi-class SVM extends to open-set recognition and applying our strategy to the formalization of that extension. 

% Open set recognition is the problem of recognizing instances of classes of objects without knowing the classifications of all candidate objects. It brings the classification problem to the real world, where the objects present are dynamic and potentially novel examples are possible at any time. "It is somewhat reasonable to assume we can gather examples of the positive class, but the number and variety of 'negatives' is not well modelled"(TODO: Remove Quotes)\cite{scheirer_toward_2013}. Open set recognition has primarily been examined in computer vision object and face recognition using linear SVMs \cite{scheirer_toward_2013} \cite{bendale_towards_2015}, where its useful to introduce the concept of "open space risk" -- a metric that increases as a sample is on the correct side, but further from, the SVM boundary plane. Scheirer et al. \cite{scheirer_toward_2013} addresses this by introducing a second plane to the SVM that attempts to put a bounding on the "backside" of a classified group, and examines how this method varies in effectiveness with the openness \footnote{Where openness is a measure of how many more classes there are at test time than at training time.} of the problem. Further work is done on recognizing and categorizing novel classes \cite{bendale_towards_2015} as well as using probabilistic techniques and nonlinear SVM kernels to classify and recognize different categories \cite{scheirer_probability_2014} \cite{jain_multi-class_2014}.

\section{Problem Formalization}

We identify a set of users through their \textit{interaction fingerprint}. A person produces measurable signals when interacting with another human or agent. These signals may come in several forms and are detectable through different sensor modalities -- a person might frequently nod in response to ideas from their colleague or be prone to verbosity in their explanations. We can identify individuals by extracting features from their temporal behaviors, which we refer to as their \textit{interaction fingerprint}. A person may persist behaviors between interaction partners, and may exhibit alternate behaviors with specific partners over the course of an interaction. 

As interactions are embedded in time, we examine whether an informative interaction fingerprint can be taken from behavior durations. In addition we look at the identification accuracy and precision of feature set describing the activity, variability, and concurrency of a set of behavioral signals. We identify a set of users $u_n \subseteq U$, where the number of users in the set, $n$, are unknown. Each user exhibits interactive behaviors that are measurable as $k$ time-series signals.

\begin{equation}
    b^k \subseteq B_n
\end{equation}

\begin{equation}
    [b^k_{t=0} ... b^k_{t=n}] \in b^k
\end{equation}

Where $b^k$ is the time-series interactive signal, and $b^k_t$ indicates whether the interactive behavior is present at time-step $t$. In order to convert the time-series interactive behavior into an SVM classifiable signal, a discrete set of $i$ features are extracted from $b^k$ to form $d^k$.

\begin{equation}
    [d^k_0 ... d^k_i] \in d^k
\end{equation}

Where each entry in $d^k$ is an extracted feature that describes an aspect of the time-series signal $b^k$. We can then feed the extracted features into $n$ one-against-all Support Vector Machines which find the hyperplane that most separates the target class from the rest of the data by finding $w$ and $b$ in the following equation. \cite{abe_analysis_2003}

\begin{equation}
    D(x) = w^tx + b
\end{equation}

Where $w$ is a $k*i$ (the number of time-series signals times the number of features extracted from each signal) dimensional vector and $b$ is a scalar. These values are found by training on a labelled subset of the data. 


\section{Methodology}
We take time-series data from recorded, annotated meetings, cut it into multiple training and test samples, perform feature extraction on each sample, and train a binary SVM classifier for each pair of users. We hypothesized that we would extract interaction fingerprint descriptive enough to accurately classify each person. 


\subsection{Data Collection - AMI Corpus}
We used the AMI Meeting Corpus \cite{kilgour_ami_nodate} to test the efficacy of our classification scheme. The AMI corpus is multi-modal dataset consisting of video and audio recordings of mock and real meetings. The AMI corpus also contains automatic and manual annotations taken over the raw recordings. Participants are assigned roles that determine their dialogue and activity within the meetings.

We evaluated our system on meetings which had been manually annotated for three modalities, leg movement, head movement, and dialogue transcripts. Each datum is associated with a timespan and may be tagged as a type when relevant (e.g. a head movement may be tagged as a 'shake', 'nod', or untagged)

\subsection{Feature Engineering}\label{feature_engineering}
Interaction data is time-series by its nature, so in order to classify we need to extract features that represent snippets of time as single values.  

To build a system that generalizes across interaction types, the signals making up $B_n$ are processed without taking their contextual meaning into account. Each signal in $B_n$ occurs over a variable time period. In a given time-step of the interaction a signal will only be either present or absent. Using this information we can classify with several features:

\begin{enumerate}
    \item Active Ratio - For what percentage of the snippet was the signal active (on versus off).
    \item Average Oscillations - How many times did the signal toggle from on to off (normalized by snippet duration). 
    \item Signal Concurrency - For what percentage of the snippet are multiple signals active at the same time. 
\end{enumerate}

The Active Ratio and Average Oscillation features produce one value for each signal in $B_n$ and were chosen because of their simplicity and straightforward meaning. Signal Concurrency attempts to capture a relationship between signals based on the intuition that different individuals may coordinate behaviors between modalities, for example a person who consistently vocalizes an "mmhmm" while nodding. 

% \meta{Frequency domain - connecting different signals. Spectral analysis.} \\
% \meta{Covariance matrix - Eigenvectors / Eigenvalues and how they relate different variables to each other.}

\subsection{Experimental Design}

We selected eight different meetings from the AMI corpus that had manually annotated leg movements, head movements, and dialogue for each participant. Each meeting contained four unique individuals, no individual participated in multiple meetings. The annotations were extracted into data streams for each participants and split into ten, equal length segments. 75\% of the data was designated as training data and paired with the corresponding participant label while the remaining 25\% was designated as a testing set. A scikit-learn SVM SVC one-vs-one classifier was then trained and used to classify the test set. Each participant was classified within their own meeting, and not between meetings. We performed 20-fold validation in each case. We examined how this standard SVM classifier performed with different feature sets and for different interaction sample durations. 

We performed an ablation study for the hand-crafted features in section \ref{feature_engineering}. We left one feature out for each trial we ran to determine which feature had the greatest impact. We were curious whether a minimum duration of interaction was needed for accurate classifications, so each pair of features was tested for a range of sample durations between 1 and 100 seconds. The sample duration that gave the highest accuracy was used to produce an average f-score for each feature set. 

\section{Results}

We found low variation between interaction sample durations. There was an increase in accuracy around the 60 second mark, but the change was not significant. We ran the the ablation f-score study using 60 seconds becuase it was nominally the best and because one-minute of interaction is a feasible amount of time to interact with a person before recognizing them.

\begin{figure}[h!]
    \caption{Feature set vs. Average F score for all people in eight different meetings. Each person was classified within their meeting of four.}
    \centering
    \includegraphics[width=0.5\textwidth]{fscore_v_feature.png}
    \label{fscore}
\end{figure}

Our ablation study (figure \ref{fscore}) showed that Signal Concurrency was the least helpful of our three features. Our measures of signal activity and oscillation resulted in ~10\% increase in f-score over a feature set using concurrency.

\begin{figure}[h]
    \caption{Average classification accuracy by snippet duration.}
    \centering
    \includegraphics[width=0.5\textwidth]{Acc_feature_v_duration.png}
    \label{acc_snippet}
\end{figure}

\section{Discussion}

Our results show that the simplest features, activity and oscillation, which were taken from independent streams, performed better than Signal Concurrency, which attempted to connect activity between signal streams. However, it seems reasonable that better accuracy and precision can be obtained by correlating activity between behavioral signals, but our naive approach to describing this correlation did more harm than good. This may indicate that automatic time-series feature generation, of the sort Recurrent Neural Networks are good for, could result in better f-scores. It is also worth noting that the selected features were intentionally kept generic with no implicit understanding of the behavior they described, it may be reasonable to craft behavior specific features for a more specialized identification system. 

As figure \ref{acc_snippet} shows, we found that there was generally no meaningful difference in identification accuracy between different snippet durations above a threshold. A useful interaction fingerprint can be generated from our engineered features as long as a sufficient amount of the interaction is observed. This result does not necessarily hold for every kind of feature, its possible that individuals demonstrate identifying behaviors on the scale of minutes or entire interactions, e.g. the way a person leaves an interaction could produce the fingerprint that solidifies their identification. 

It's important to point out that our relatively high accuracy could be due to the AMI corpus' use of role assignments within meetings. Each person in a meeting was given a role that in part determined their behavior. We did not control for these role assignments in our study, and the role assignments may serve as a confounding variable. 

\section{Future Work}

The generic features we chose to classify upon were insufficient to give consistently accurate identifications. However, our negative results hint at promising research directions to pursue towards HRI identification schemes. The passive signals we used may still be useful, but we can gain additional information by using the agency inherent in our HRI systems. An agent can prompt a user for behavioral signatures rather than wait for the user to exhibit identifying behavior -- even negative responses to prompts may be distinguishing. 

In addition, identifying users through elicited behaviors promotes a unique one-agent-one-human relationship. An agent's ability to trigger a particular behavior may be dependent on the parameterization or even specific hardware of the agent, making it difficult to transfer the learned identification to another agent and promoting the privacy of the user. This method may also work with the reasonable person's expectation that only agents which the person has interacted with will remember them. Finally, we plan to run a study on this human-agent identification. 


% for final report
%A detailed description of your problem (with math, notation, algorithms, figures, etc.). Use footnotes to cite links to your code or videos\footnote{All developed source code for this project is available at ...}

\bibliographystyle{plain}
\bibliography{references}
\end{document}
